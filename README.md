# interpretable_tts 
# Close to Human Quality TTS with Transformer

# Paper: INTERPRETING A TEXT-TO-SPEECH MODEL BY EXPLORING AND CORRECTING VULNERABLE INTERNAL ERROR UNITS

# Authors: Seongyeop Jeong, June Sig Sung, Inchul Hwang, Jaesik Choi

# Abstract: Understanding and explaining the inner process of deep learning models remains challenging because models have numerous parameters and complex non-linear computations. It is therefore difficult to analyze and address the internal causes of artificial speech or alignment errors. In this paper, we utilize an adversarial attack method to generate artificial speech and explore vulnerabilities of the model. To detect defective units that cause errors in the model, and correct the speech by sequentially modifying internal units, a layer-wise relevance propagation (LRP) based model analysis is proposed. Through the qualitative and quantitative results, we show that LRP could be an appropriate approach to precisely capture propagated errors unit causing artificial speech in the encoder layers by evaluating attention alignment errors and mean opinion score (MOS) when internal defective units are corrected with the proposed algorithm.


# Samples generated by our model



